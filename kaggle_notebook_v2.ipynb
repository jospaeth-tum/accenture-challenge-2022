{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"datathon_SC_ACN_22/\"\n",
    "SEP = \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114276, 9) (28563, 8) (666, 5) (6660, 8) (772, 3) (772, 2)\n",
      "(114276, 12) (28563, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ports that are the same but have different names (data cleaning)\n",
    "same_ports =  {\n",
    "    'ATHENAS': 'Athens',\n",
    "    'BCN': 'Barcelona',\n",
    "}\n",
    "\n",
    "# converters function for pd.read_csv()\n",
    "convs = {\n",
    "    'origin_port': lambda x: same_ports[x] if x in same_ports else x\n",
    "}\n",
    "\n",
    "df_cities = pd.read_csv(DATA_FOLDER + \"cities_data.csv\", sep=SEP)\n",
    "df_cities_costs = pd.read_csv(DATA_FOLDER + \"cities_data_costs.csv\", sep=\",\")\n",
    "df_orders = pd.read_csv(\n",
    "  DATA_FOLDER + \"orders.csv\",\n",
    "  sep=SEP,\n",
    "  na_filter=False,\n",
    "  converters=convs)\n",
    "df_product_attr = pd.read_csv(DATA_FOLDER + \"product_attributes.csv\", sep=\",\")\n",
    "df_product_weight_class = pd.read_csv(DATA_FOLDER + \"product_weight_class.csv\", sep=\",\")\n",
    "df_test = pd.read_csv(\n",
    "  DATA_FOLDER + \"test.csv\",\n",
    "  sep=SEP,\n",
    "  na_filter=False,\n",
    "  converters=convs)\n",
    "print(df_orders.shape, df_test.shape, df_cities.shape, df_cities_costs.shape, df_product_attr.shape, df_product_weight_class.shape)\n",
    "\n",
    "def join_with_product_attr(df, mean_weight=None):\n",
    "  # the inner merge operation would get rid of orders that have product_id -1\n",
    "  df_out = df.merge(df_product_attr, on=\"product_id\", how=\"left\")\n",
    "\n",
    "  # fill NaN values\n",
    "  df_out[\"material_handling\"] = df_out[\"material_handling\"].fillna(-1)\n",
    "\n",
    "  # use mean of weights for missing weights\n",
    "  if mean_weight is None:\n",
    "    mean_weight = df_out[\"weight\"].mean()\n",
    "  df_out[\"weight\"] = df_out[\"weight\"].fillna(mean_weight)\n",
    "\n",
    "  # multiply units with weight, giving total weight\n",
    "  df_out[\"total_weight\"] = df_out[\"units\"] * df_out[\"weight\"]\n",
    "\n",
    "  return df_out, mean_weight\n",
    "\n",
    "# join order data with product attributes\n",
    "df_orders, mean_weight = join_with_product_attr(df_orders)\n",
    "df_test, _ = join_with_product_attr(df_test, mean_weight)\n",
    "\n",
    "print(df_orders.shape, df_test.shape)\n",
    "\n",
    "# sort values of test dataset, which will help later with creating the datasets for ML\n",
    "df_orders.sort_values(\"late_order\", inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size: (114276, 820)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def make_discrete_features(df, discrete_cols, enc, fit):\n",
    "  discrete_raw_features = df[discrete_cols]\n",
    "  if fit:\n",
    "    enc.fit(discrete_raw_features)\n",
    "  discrete_features = np.array(enc.transform(discrete_raw_features).toarray())\n",
    "  return discrete_features, enc\n",
    "\n",
    "one_hot_features, enc = make_discrete_features(\n",
    "  df_orders,\n",
    "  [\n",
    "    \"origin_port\",\n",
    "    \"3pl\",\n",
    "    \"customs_procedures\",\n",
    "    \"logistic_hub\",\n",
    "    \"customer\",\n",
    "    \"product_id\",\n",
    "  ],\n",
    "  OneHotEncoder(),\n",
    "  fit=True)\n",
    "\n",
    "# scale the unit values\n",
    "unscaled_units = df_orders[\"units\"].to_numpy()[:, np.newaxis]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(unscaled_units)\n",
    "units_scaled = scaler.transform(unscaled_units)\n",
    "# print(units_scaled)\n",
    "\n",
    "# scale the total_weight values\n",
    "unscaled_total_weight = df_orders[\"total_weight\"].to_numpy()[:, np.newaxis]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(unscaled_total_weight)\n",
    "total_weight_scaled = scaler.transform(unscaled_total_weight)\n",
    "\n",
    "raw_X = np.concatenate([one_hot_features, units_scaled, total_weight_scaled], axis=1) \n",
    "raw_y = df_orders[\"late_order\"]\n",
    "# print(raw_X)\n",
    "print(\"Feature size:\", raw_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114276 total\n",
      "{'0': 87120, '1': 27156} -> 0.7623648010080857 negative\n",
      "X: (108624, 820) y: (108624,)\n",
      "ratio: 0.5\n",
      "split at 76036\n"
     ]
    }
   ],
   "source": [
    "### Balance out features and targets\n",
    "\n",
    "# count amount of negatives\n",
    "num_positives = sum(raw_y)\n",
    "num_negatives = len(raw_y) - num_positives\n",
    "\n",
    "assert sum(raw_y[:num_negatives]) == 0\n",
    "assert sum(raw_y[num_negatives:]) == num_positives\n",
    "\n",
    "print(len(raw_y), \"total\")\n",
    "print({\n",
    "  \"0\": num_negatives,\n",
    "  \"1\": num_positives\n",
    "}, \"->\", num_negatives / len(raw_y), \"negative\")\n",
    "\n",
    "# select negative features\n",
    "perm = np.random.permutation(num_negatives)\n",
    "negative_X = raw_X[perm[:num_positives*2]]\n",
    "negative_y = raw_y[perm[:num_positives*2]]\n",
    "\n",
    "# select positive features\n",
    "positive_X = raw_X[num_negatives:]\n",
    "positive_y = raw_y[num_negatives:]\n",
    "\n",
    "# concatenate features\n",
    "X = np.concatenate((negative_X, positive_X, positive_X), axis=0)\n",
    "y = np.concatenate((negative_y, positive_y, positive_y), axis=0)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"ratio:\", sum(y) / len(y))\n",
    "\n",
    "# shuffle datasets\n",
    "shuffle_perm = np.random.permutation(len(X))\n",
    "X = X[shuffle_perm]\n",
    "y = y[shuffle_perm]\n",
    "\n",
    "split = (len(y) * 7) // 10\n",
    "print(\"split at\", split)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train models\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# svm_model = SVC(gamma=\"auto\")\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "# TODO: adjust data for tree and output tree\n",
    "# X_train_tree = np.concatenate([\n",
    "#   enc.inverse_transform(X_train[:, :-1]),\n",
    "#   X_train[:, -1:]], axis=1)\n",
    "\n",
    "# X_test_tree = np.concatenate([\n",
    "#   enc.inverse_transform(X_test[:, :-1]),\n",
    "#   X_test[:, -1:]], axis=1)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "log_model = LogisticRegression(solver=\"sag\")\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred [ True  True  True ...  True  True  True]\n",
      "pred ratio 0.5652080520436971\n",
      "tree acc: 0.8111881674235915\n",
      "pred [ True False  True ...  True  True  True]\n",
      "pred ratio 0.4878482877132687\n",
      "log acc: 0.7484963790352277\n"
     ]
    }
   ],
   "source": [
    "# evaluate the approaches\n",
    "def eval_model(pred_fn, name):\n",
    "  pred = pred_fn(X_test)\n",
    "  print(\"pred\", pred)\n",
    "  print(\"pred ratio\", sum(pred) / len(X_test))\n",
    "  hits = sum([1 if y_hat == y else 0 for (y_hat, y) in zip(pred, y_test) ])\n",
    "  print(name, \"acc:\", hits / len(pred))\n",
    "\n",
    "# eval_model(svm_model.predict, \"svm\")\n",
    "\n",
    "eval_model(tree_model.predict, \"tree\")\n",
    "# simple: 0.80\n",
    "\n",
    "eval_model(log_model.predict, \"log\")\n",
    "# simple: 0.75\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size: (28563, 820)\n",
      "[[0.90244219 0.09755781]\n",
      " [0.90158268 0.09841732]\n",
      " [0.77153897 0.22846103]\n",
      " ...\n",
      " [0.84886028 0.15113972]\n",
      " [0.4377506  0.5622494 ]\n",
      " [0.7347803  0.2652197 ]]\n"
     ]
    }
   ],
   "source": [
    "# make features with testing data\n",
    "one_hot_features, _ = make_discrete_features(\n",
    "  df_test,\n",
    "  [\n",
    "    \"origin_port\",\n",
    "    \"3pl\",\n",
    "    \"customs_procedures\",\n",
    "    \"logistic_hub\",\n",
    "    \"customer\",\n",
    "    \"product_id\",\n",
    "  ],\n",
    "  enc,\n",
    "  fit=False)\n",
    "\n",
    "# scale the unit values\n",
    "unscaled_units = df_test[\"units\"].to_numpy()[:, np.newaxis]\n",
    "units_scaled = scaler.transform(unscaled_units)\n",
    "\n",
    "# scale the unit values\n",
    "unscaled_total_weight = df_test[\"total_weight\"].to_numpy()[:, np.newaxis]\n",
    "total_weight_scaled = scaler.transform(unscaled_total_weight)\n",
    "\n",
    "X_eval = np.concatenate([one_hot_features, units_scaled, total_weight_scaled], axis=1) \n",
    "print(\"Feature size:\", X_eval.shape)\n",
    "\n",
    "# evaluate test cases\n",
    "probs = log_model.predict_proba(X_eval)\n",
    "print(probs)\n",
    "\n",
    "# print probabilities\n",
    "submission = pd.DataFrame({\"order_id\": df_test.order_id, \"late_order\": probs[:,1]})\n",
    "submission.to_csv(\"submission/submission_kaggle_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
